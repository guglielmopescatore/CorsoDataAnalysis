{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "572591d9",
   "metadata": {},
   "source": [
    "\n",
    "# Lecture 2: Methods of Data Collection and Data Cleaning Techniques\n",
    "Date: [Insert Date]  \n",
    "Duration: 3 hours\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Methods of Data Collection](#Methods-of-Data-Collection)\n",
    "   - [Web Scraping](#Web-Scraping)\n",
    "   - [APIs](#APIs)\n",
    "   - [Public Datasets](#Public-Datasets)\n",
    "3. [Data Cleaning Techniques](#Data-Cleaning-Techniques)\n",
    "   - [Handling Missing Values](#Handling-Missing-Values)\n",
    "   - [Text Cleaning](#Text-Cleaning)\n",
    "   - [Audio and Video Preprocessing](#Audio-and-Video-Preprocessing)\n",
    "4. [Exploratory Data Analysis (EDA)](#Exploratory-Data-Analysis-(EDA))\n",
    "   - [Basics of Data Visualization](#Basics-of-Data-Visualization)\n",
    "   - [Statistical Summary](#Statistical-Summary)\n",
    "5. [Q&A and Discussion](#QnA-and-Discussion)\n",
    "6. [Conclusion](#Conclusion)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This lecture will focus on the various methods of data collection, including web scraping, APIs, and public datasets, followed by a section on data cleaning techniques, and concluding with an introduction to exploratory data analysis (EDA). We will learn about the tools and considerations for each method, and practice techniques to prepare data for analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901b38aa",
   "metadata": {},
   "source": [
    "## Methods of Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919091ed",
   "metadata": {},
   "source": [
    "\n",
    "### Web Scraping\n",
    "Web scraping is a method used to extract data from websites. This technique is useful when there is a large amount of data on a website that is not readily available in a downloadable format. Python provides several libraries to perform web scraping, the most popular ones being BeautifulSoup and Scrapy.\n",
    "\n",
    "In this section, we will demonstrate how to scrape the episode data for 'Grey's Anatomy' from a Fandom website using BeautifulSoup and requests. The goal is to collect the following information:\n",
    "\n",
    "- Number in series\n",
    "- Number in season\n",
    "- Title\n",
    "- Original air date\n",
    "- U.S. viewers (millions)\n",
    "\n",
    "**Note**: Web scraping must be performed in compliance with the website's terms of service. Always check the website's `robots.txt` file and terms of service to ensure that you are allowed to scrape their data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fd3751",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Required libraries for web scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf34202f",
   "metadata": {},
   "source": [
    "\n",
    "#### Fetching Webpage Content\n",
    "To scrape data from a web page, we first need to download the page. This is done by making a GET request to the website's server to retrieve the HTML content. Here, we would typically use the `requests` library to make this request. The `requests.get()` method allows us to send an HTTP GET request to the URL of the webpage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a68532",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# URL of the page we want to scrape\n",
    "url = 'https://greysanatomy.fandom.com/wiki/Grey%27s_Anatomy_Episodes'\n",
    "\n",
    "# Perform the GET request to fetch the HTML content of the page\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check the response status code to ensure the request was successful\n",
    "if response.status_code == 200:\n",
    "    html_content = response.text\n",
    "else:\n",
    "    html_content = \"Request failed; cannot fetch the page.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde55a01",
   "metadata": {},
   "source": [
    "\n",
    "#### Parsing HTML Content\n",
    "Once we have the HTML content of the page, we can use BeautifulSoup to parse this content and extract the data we need. BeautifulSoup transforms a complex HTML document into a complex tree of Python objects. The two most common objects are:\n",
    "\n",
    "- `Tag`: Corresponds to an HTML tag in the original document. These are navigable and can be searched.\n",
    "- `NavigableString`: Represents the text within the tags, which is what we are often trying to extract.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df5b61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The following code would parse the HTML content received from the GET request.\n",
    "# Since we are using a pre-downloaded HTML file, this step is not needed in this environment.\n",
    "\n",
    "# Parse the HTML content with BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Now 'soup' contains the parsed HTML content ready for data extraction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742b4186",
   "metadata": {},
   "source": [
    "\n",
    "#### Extracting Data\n",
    "After parsing the HTML content, we can use BeautifulSoup to navigate the parse tree and extract the parts of the HTML that contain the data we are interested in. The data is typically contained within HTML tags, and we can use methods like `find_all()` to search for specific tags, or navigate the tree using tag names as attributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5edf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The actual scraping logic to extract data from the HTML content\n",
    "# Here we loop over each table, find all the rows within, and then extract each cell's content.\n",
    "\n",
    "# Find all the tables on the page\n",
    "tables = soup.find_all('table')\n",
    "\n",
    "# We will extract data from the first 19 tables as per the previous example.\n",
    "# Define a list to store data for all episodes\n",
    "all_episodes = []\n",
    "\n",
    "for table in tables[:19]:\n",
    "    rows = table.find_all('tr')[1:]  # Skip the header row\n",
    "    for row in rows:\n",
    "        cells = row.find_all(['th', 'td'])\n",
    "        if len(cells) == 5:\n",
    "            episode_data = {\n",
    "                'No. in series': cells[0].text.strip(),\n",
    "                'No. in season': cells[1].text.strip(),\n",
    "                'Title': cells[2].text.strip().strip('\"'),  # Remove potential quotes around the title\n",
    "                'Original air date': cells[3].text.strip(),\n",
    "                'U.S. viewers (millions)': cells[4].text.strip().split('[')[0]  # Remove reference links\n",
    "            }\n",
    "            all_episodes.append(episode_data)\n",
    "\n",
    "# Convert to a pandas DataFrame\n",
    "episodes_df = pd.DataFrame(all_episodes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ae2fd0",
   "metadata": {},
   "source": [
    "\n",
    "#### Saving Data\n",
    "After extracting the data and placing it into a structured format, such as a pandas DataFrame, we can save it to a file for later use. DataFrames can be easily exported to various formats including CSV, Excel, JSON, etc. using methods like `to_csv()`, `to_excel()`, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252ccdf6",
   "metadata": {},
   "source": [
    "#### Full code ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac068ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# URL of the page we want to scrape\n",
    "url = 'https://greysanatomy.fandom.com/wiki/Grey%27s_Anatomy_Episodes'\n",
    "\n",
    "# Perform the GET request to fetch the HTML content of the page\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check the response status code to ensure the request was successful\n",
    "if response.status_code == 200:\n",
    "    html_content = response.text\n",
    "else:\n",
    "    html_content = \"Request failed; cannot fetch the page.\"\n",
    "\n",
    "\n",
    "\n",
    "# The following code would parse the HTML content received from the GET request.\n",
    "# Since we are using a pre-downloaded HTML file, this step is not needed in this environment.\n",
    "\n",
    "# Parse the HTML content with BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Now 'soup' contains the parsed HTML content ready for data extraction.\n",
    "\n",
    "\n",
    "\n",
    "# The actual scraping logic to extract data from the HTML content\n",
    "# Here we loop over each table, find all the rows within, and then extract each cell's content.\n",
    "\n",
    "# Find all the tables on the page\n",
    "tables = soup.find_all('table')\n",
    "\n",
    "# We will extract data from the first 19 tables as per the previous example.\n",
    "# Define a list to store data for all episodes\n",
    "all_episodes = []\n",
    "\n",
    "for table in tables[:19]:\n",
    "    rows = table.find_all('tr')[1:]  # Skip the header row\n",
    "    for row in rows:\n",
    "        cells = row.find_all(['th', 'td'])\n",
    "        if len(cells) == 5:\n",
    "            episode_data = {\n",
    "                'No. in series': cells[0].text.strip(),\n",
    "                'No. in season': cells[1].text.strip(),\n",
    "                'Title': cells[2].text.strip().strip('\"'),  # Remove potential quotes around the title\n",
    "                'Original air date': cells[3].text.strip(),\n",
    "                'U.S. viewers (millions)': cells[4].text.strip().split('[')[0]  # Remove reference links\n",
    "            }\n",
    "            all_episodes.append(episode_data)\n",
    "\n",
    "# Convert to a pandas DataFrame\n",
    "episodes_df = pd.DataFrame(all_episodes)\n",
    "\n",
    "\n",
    "episodes_df.to_csv('episodes_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc8d875",
   "metadata": {},
   "source": [
    "\n",
    "### APIs (Application Programming Interfaces)\n",
    "An API is a set of rules that allows one software application to interact with another. They define the correct way for a developer to write a program that requests services from an operating system or other application.\n",
    "\n",
    "In the context of data analysis for media, APIs can be used to collect a vast array of data from social media platforms, video services, and other online sources. A popular video-related API is the YouTube Data API, which provides access to YouTube video and channel data.\n",
    "\n",
    "**Note**: To use the YouTube Data API, you need to have an API key which can be obtained through the Google Developer Console.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86d436c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Libraries required for interacting with the YouTube Data API\n",
    "import os\n",
    "from googleapiclient.discovery import build\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6126f1fb",
   "metadata": {},
   "source": [
    "\n",
    "#### Fetching Data from YouTube Data API\n",
    "To fetch data from the YouTube Data API, we need to build a service object using the `build` function provided by `googleapiclient.discovery`. This service object is then used to make requests to the API.\n",
    "\n",
    "Below is a code example that would fetch the most popular videos on YouTube in a specific region and category. The `videos().list()` method is used with the `part='snippet,statistics'` parameter to get basic details about each video along with its statistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9778ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Since this environment does not allow internet access, the following code is for illustrative purposes only.\n",
    "\n",
    "# You would need to replace 'YOUR_API_KEY' with your actual API key.\n",
    "#api_key = 'YOUR_API_KEY'\n",
    "api_key = 'AIzaSyCMFiSJeVqMoxl8aFMnZfX-c_7abGdFIs0'\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "# This function will fetch the most popular videos in the United States for a specific category\n",
    "def get_popular_videos(regionCode='US', categoryId='10', maxResults=50):\n",
    "    response = youtube.videos().list(\n",
    "        part='snippet,statistics',\n",
    "        chart='mostPopular',\n",
    "        regionCode=regionCode,\n",
    "        videoCategoryId=categoryId,\n",
    "        maxResults=maxResults\n",
    "    ).execute()\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42b1ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "popular_videos = get_popular_videos()\n",
    "# The `popular_videos` variable would contain the response from the API\n",
    "\n",
    "# Pretty print the JSON data with an indentation of 2 spaces\n",
    "pretty_popular_videos = json.dumps(popular_videos, indent=2)\n",
    "print(pretty_popular_videos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534eb154",
   "metadata": {},
   "source": [
    "### Public Datasets\n",
    "The IMDb public datasets are subsets of data from the IMDb database that are available for personal and non-commercial use. They are updated daily and provided in gzipped, tab-separated-values (TSV) format in UTF-8 character set. \n",
    "The IMDb datasets can be accessed and downloaded from the following URL: https://datasets.imdbws.com/.\n",
    "\n",
    "These datasets include:\n",
    "\n",
    "1. **title.akas.tsv.gz**: Contains alternative titles for movies with details such as the region and language, and attributes like whether it is the original title.\n",
    "\n",
    "2. **title.basics.tsv.gz**: Includes basic information about titles like type of title, primary and original title, release year, runtime, and genres.\n",
    "\n",
    "3. **title.crew.tsv.gz**: Lists the directors and writers associated with a title using IMDb's unique identifier system.\n",
    "\n",
    "4. **title.episode.tsv.gz**: Provides information about TV series episodes, including season and episode number.\n",
    "\n",
    "5. **title.principals.tsv.gz**: Contains information about the main participants in a title, such as cast and crew, and their roles.\n",
    "\n",
    "6. **title.ratings.tsv.gz**: Shows the average rating and number of votes for titles.\n",
    "\n",
    "7. **name.basics.tsv.gz**: Contains information about individuals involved in film and television, including their known-for titles and primary professions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221643be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import requests\n",
    "# import gzip\n",
    "# from io import StringIO\n",
    "\n",
    "# # Function to download and decompress gzipped datasets\n",
    "# def download_decompress_imdb_dataset(url):\n",
    "#     response = requests.get(url)\n",
    "#     gzip_file = gzip.GzipFile(fileobj=StringIO(response.content))\n",
    "#     return pd.read_csv(gzip_file, sep='\\t', low_memory=False, dtype=str)\n",
    "\n",
    "# # URLs for the datasets\n",
    "# episode_url = 'https://datasets.imdbws.com/title.episode.tsv.gz'\n",
    "# principals_url = 'https://datasets.imdbws.com/title.principals.tsv.gz'\n",
    "\n",
    "# # Download and decompress the datasets\n",
    "# title_episode_df = download_decompress_imdb_dataset(episode_url)\n",
    "# title_principals_df = download_decompress_imdb_dataset(principals_url)\n",
    "\n",
    "# # Merge the datasets on the title identifier 'tconst'\n",
    "# merged_df = pd.merge(title_episode_df, title_principals_df, on='tconst')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549d12b5",
   "metadata": {},
   "source": [
    "\n",
    "### Data Cleaning Techniques\n",
    "Data cleaning is the process of preparing data for analysis by removing or modifying data that is incorrect, incomplete, irrelevant, duplicated, or improperly formatted. This is a fundamental step since the quality of data and the amount of useful information that can be derived from it directly affects the ability of our analysis to be accurate and reliable.\n",
    "\n",
    "#### Handling Missing Values\n",
    "Missing data can occur when no information is provided for one or more items or for a whole unit. Missing data is a common issue in data analysis and can be dealt with in several ways:\n",
    "\n",
    "- **Dropping**: Removing the rows or columns with missing values.\n",
    "- **Filling**: Replacing missing values with a specific value or a statistic (mean, median, etc.).\n",
    "- **Interpolation**: Estimating the missing values from other data points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46579f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the cleaning file\n",
    "GA_cleaning_df = pd.read_csv('GA_cleaning.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f053ed4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First, let's check for missing values in our dataset\n",
    "missing_values_count = GA_cleaning_df.isnull().sum()\n",
    "missing_values_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e31812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the missing values are not significant, we can choose to drop them\n",
    "# Here's how to drop rows with any missing values\n",
    "GA_cleaned_df = GA_cleaning_df.dropna()\n",
    "\n",
    "# Check the result\n",
    "print(GA_cleaned_df.isnull().sum())\n",
    "GA_cleaned_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57df4a1",
   "metadata": {},
   "source": [
    "\n",
    "#### Cleaning the \"U.S. viewers (millions)\" Column\n",
    "The \"U.S. viewers (millions)\" column is expected to contain numerical values representing the viewership in millions. However, due to the nature of web scraping and the variability of data on web pages, this column may have been imported as a string type and could contain non-numeric characters or missing values. \n",
    "\n",
    "To ensure that we can perform numerical operations on this data, we need to clean this column by:\n",
    "- Converting the column to a numeric type using `pd.to_numeric()`.\n",
    "- Handling any non-numeric values or strings that cannot be converted to numbers.\n",
    "- Filling in any remaining missing values with an appropriate statistic such as the mean or median.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7c33a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the DataFrame slice\n",
    "GA_cleaned_df = GA_cleaned_df.copy()\n",
    "\n",
    "# Remove unwanted characters from 'U.S. viewers (millions)' column using regular expression\n",
    "GA_cleaned_df['U.S. viewers (millions)'] = GA_cleaned_df['U.S. viewers (millions)'].str.extract(r'([\\d\\.]+)')\n",
    "\n",
    "# Convert the 'U.S. viewers (millions)' column to numeric\n",
    "GA_cleaned_df['U.S. viewers (millions)'] = pd.to_numeric(GA_cleaned_df['U.S. viewers (millions)'], errors='coerce')\n",
    "\n",
    "# Display the head of the cleaned DataFrame\n",
    "GA_cleaned_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d46aac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Interpolation is a method that can be used to estimate missing values between two known value\n",
    "# Since 'No. in season' is a numerical but ordinal column, interpolation doesn't make sense for it\n",
    "# Therefore, we will not perform interpolation in this dataset as an example\n",
    "# But here is how you would generally use interpolation for an interval or ratio scale column\n",
    "# cleaned_df_interpolate = cleaning_df.interpolate(method='linear')\n",
    "\n",
    "# Check the result (Note: We don't actually perform interpolation here, so this is just illustrative)\n",
    "# cleaned_df_interpolate.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5dfa6f",
   "metadata": {},
   "source": [
    "\n",
    "### Exploratory Data Analysis (EDA) with pandas-profiling\n",
    "Exploratory Data Analysis (EDA) is an important step in any data analysis process. It allows us to understand the data, find patterns and anomalies, and form hypotheses based on the features of the data.\n",
    "\n",
    "`ydata-profiling` is a library that creates a detailed report for a given DataFrame. It's a great tool for EDA because it automatically computes a lot of useful information that would otherwise require a significant amount of manual work. This includes:\n",
    "\n",
    "- Descriptive statistics for each column\n",
    "- Correlations between variables\n",
    "- Histograms of the distributions of variables\n",
    "- Detection of missing values, and much more.\n",
    "\n",
    "**Note**: The `ydata-profiling` library needs to be installed in your Python environment. You can install it using `!pip install ydata-profiling` in a Jupyter Notebook cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ada9bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ydata_profiling import ProfileReport\n",
    "profile = ProfileReport(GA_cleaned_df, title='GA Episodes Report', explorative=True)\n",
    "profile.to_widgets()\n",
    "profile.to_file(\"your_report.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0f990f",
   "metadata": {},
   "source": [
    "## Q&A and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628a07ac",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
