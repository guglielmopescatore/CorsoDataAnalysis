{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b30ee915",
   "metadata": {},
   "source": [
    "# Lecture 8: Assessing Gender Gap Off-Screen in Serial Production (Part 2)\n",
    "Date: December 1, 2023\n",
    "Duration: 3 hours\n",
    "\n",
    "## Outline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15bc37d",
   "metadata": {},
   "source": [
    "### Data scraping, cleaning and preprocessing (1 hour)\n",
    "- Scraping new IMDb page\n",
    "- Handling missing values, outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc88a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74996b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the ChromeDriver (if not added to PATH)\n",
    "chrome_driver_path = \"./chromedriver\"\n",
    "chrome_binary_path = \"./chrome-linux64/chrome\"\n",
    "\n",
    "# Create a Service object\n",
    "service = Service(chrome_driver_path)\n",
    "\n",
    "# Set up Chrome options for headless mode\n",
    "chrome_options = Options()\n",
    "chrome_options.binary_location = chrome_binary_path  # Specify the path to Chrome binary\n",
    "chrome_options.add_argument(\"--headless\")  # Run Chrome in headless mode\n",
    "chrome_options.add_argument(\"--no-sandbox\")  # Bypass OS security model\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")  # Overcome limited resource problems\n",
    "chrome_options.add_argument(\"--window-size=1920,1080\")  # Set window size\n",
    "chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36\")\n",
    "\n",
    "# Initialize the WebDriver instance using the Service object and Chrome options\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62de8a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self, gen):\n",
    "        self.gen = gen\n",
    "        self.value = None\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.value = yield from self.gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32204aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to load an image for the sidebar\n",
    "def load_image(image_path):\n",
    "    with open(image_path, 'rb') as file:\n",
    "        img = Image.open(file)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd7f3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_imdb(url):\n",
    "    # Load the IMDb page\n",
    "    driver.get(url)  # Replace with your IMDb URL\n",
    "\n",
    "    # Initialize WebDriverWait\n",
    "    wait = WebDriverWait(driver, 5)\n",
    "\n",
    "    # Wait for the initial page to load\n",
    "    time.sleep(4)\n",
    "\n",
    "    # Extract the total number of results using the updated method\n",
    "    try:\n",
    "        total_results_element = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '.sc-54d06b29-3')))\n",
    "        total_results_str = total_results_element.text\n",
    "        # Extracting the number from the first line\n",
    "        first_line = total_results_str.split('\\n')[0]\n",
    "        total_results_number_str = first_line.split()[-1].replace('.', '').replace(',', '')\n",
    "    except NoSuchElementException:\n",
    "        print(\"Total number of results not found.\")\n",
    "        exit()\n",
    "    except TimeoutException:\n",
    "        print(\"Timeout reached extracting the total number of results.\")\n",
    "        exit()\n",
    "\n",
    "    if total_results_number_str.isdigit():\n",
    "        total_results = int(total_results_number_str)\n",
    "    else:\n",
    "        raise ValueError(\"Unable to extract total number of results as an integer\")\n",
    "\n",
    "\n",
    "    loaded_results = 0\n",
    "    # Click the \"50 more\" button until all results are loaded\n",
    "    while loaded_results < total_results:\n",
    "        try:\n",
    "            load_more_button = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '.ipc-see-more__text')))\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView(true);\", load_more_button)\n",
    "\n",
    "            # Use JavaScript to click the button\n",
    "            driver.execute_script(\"arguments[0].click();\", load_more_button)\n",
    "\n",
    "            time.sleep(3)  # Wait for the page to load more results\n",
    "            results_text = driver.find_element(By.CSS_SELECTOR, '.sc-54d06b29-3').text\n",
    "            first_line = results_text.split('\\n')[0]\n",
    "            loaded_results_str = first_line.split('-')[-1].split()[0].replace('.', '').replace(',', '')\n",
    "            if loaded_results_str.isdigit():\n",
    "                loaded_results = int(loaded_results_str)\n",
    "                # Calculate progress percentage\n",
    "                progress_fraction = (loaded_results / total_results)\n",
    "                yield progress_fraction\n",
    "            else:\n",
    "                raise ValueError(\"Unable to extract loaded results number as an integer\")\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "            break\n",
    "\n",
    "\n",
    "    # Extract the HTML content\n",
    "    html_content = driver.page_source\n",
    "    # Create a BeautifulSoup object for parsing the HTML\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Use a set to store IMDB codes and ensure uniqueness\n",
    "    imdb_codes_set = set()\n",
    "    for tag in soup.find_all('a', href=True):\n",
    "        if 'title/tt' in tag.get('href'):\n",
    "            code = tag.get('href').split('/')[2].split('?')[0]\n",
    "            imdb_codes_set.add(code)\n",
    "\n",
    "    # Convert the set to a list for the DataFrame\n",
    "    imdb_codes_list = list(imdb_codes_set)\n",
    "\n",
    "    # Close the Selenium browser\n",
    "    driver.quit()\n",
    "    return imdb_codes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff395d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Sidebar with logo and instructions\n",
    "    st.sidebar.image(load_image('logo_IMDb_scraper.webp'), use_column_width=True)\n",
    "    st.sidebar.info(\"\"\"\n",
    "            Welcome to the IMDb scraping tool. To extract titles and related data from IMDb using an advanced search URL, follow these steps:\n",
    "\n",
    "            1. Perform an advanced search on IMDb (IMDb Advanced Search) with desired criteria like genre, year, rating, etc.\n",
    "            2. Copy the URL from the browser's address bar after viewing the results.\n",
    "            3. Paste the copied URL into the designated field in the web app.\n",
    "            4. Click the \"Start Scraping\" button to initiate data extraction from the listed titles.\n",
    "            5. Upon completion, the results can be downloaded in CSV.\n",
    "\n",
    "            Note: The search URL can be directly manipulated by using '!' to exclude parameters (e.g., 'genres=!drama') and including categories like 'country_of_origin' or 'primary_language' to personalize search parameters not available in IMDb's standard interface. Respect IMDb's terms of service and legal restrictions on web scraping.\n",
    "        \"\"\")\n",
    "\n",
    "    # Main app interface\n",
    "    st.title(\"IMDb Web Scraper\")\n",
    "\n",
    "    # Text input for URL\n",
    "    url = st.text_input(\"Enter the IMDb URL to scrape:\")\n",
    "\n",
    "    # Container for messages and progress bar\n",
    "    status_container = st.empty()\n",
    "\n",
    "\n",
    "    # Button to start scraping\n",
    "    if st.button(\"Start Scraping\") and url:\n",
    "        progress_bar = st.progress(0)\n",
    "        gen_wrapper = Generator(scrape_imdb(url))\n",
    "\n",
    "        for progress in gen_wrapper:\n",
    "            progress_bar.progress(progress)\n",
    "\n",
    "        scraped_data = gen_wrapper.value\n",
    "\n",
    "        if scraped_data:\n",
    "            status_container.success(\"Scraping Completed!\")\n",
    "            results_df = pd.DataFrame({'IMDB_Code': scraped_data})\n",
    "            csv_data = results_df.to_csv(index=False).encode('utf-8')\n",
    "            st.download_button(label=\"Download Data as CSV\", data=csv_data, file_name=\"scraped_data.csv\",\n",
    "                                   mime=\"text/csv\")\n",
    "        else:\n",
    "            status_container.error(\"No data scraped.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f81a7c",
   "metadata": {},
   "source": [
    "### Data analysis techniques (1 hour)\n",
    "- Descriptive statistics, hypothesis testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f58d6dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import PySimpleGUI as sg\n",
    "import dns\n",
    "import copy\n",
    "import json\n",
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "pbar = ProgressBar()\n",
    "pbar.register()\n",
    "import pymongo\n",
    "import multiprocessing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import imdb\n",
    "from imdb import IMDb, IMDbError\n",
    "ia = IMDb()\n",
    "CPU_COUNT = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5edd9fe",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_table():\n",
    "\n",
    "    #sg.set_options(auto_size_buttons=True)\n",
    "    filename = sg.popup_get_file(\n",
    "        'Dataset to read',\n",
    "        #no_titlebar=True,\n",
    "        #grab_anywhere=True,\n",
    "        file_types=((\"CSV Files\", \"*.csv\"),),\n",
    "        )\n",
    "\n",
    "    if not filename:\n",
    "        sg.popup(\"No filename supplied, exit\")\n",
    "        raise SystemExit(\"Cancelling: no filename supplied\")\n",
    "\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19dfe0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_infoset():\n",
    "    infolist = ia.get_movie_infoset()\n",
    "    unwanted_infosets = {'main', 'news', 'soundtrack'}\n",
    "    infolist = [ele for ele in infolist if ele not in unwanted_infosets]\n",
    "    #define layout\n",
    "    layout=[[sg.Text('The default infoset is main \\n You can add more infosets',size=(30, 2), justification='left')],\n",
    "        [sg.Listbox(infolist, default_values='', select_mode='extended', key='info', size=(30, 8))],\n",
    "        [sg.Button('SAVE'), sg.Button('CANCEL')]]\n",
    "    \n",
    "    #Define Window\n",
    "    win =sg.Window('Additional infosets',layout)\n",
    "    \n",
    "    #Read  values entered by user\n",
    "    e,v=win.read()\n",
    "    strv = \", \".join(v['info'])\n",
    "    #close first window\n",
    "    win.close()\n",
    "    #display string in a popup         \n",
    "    sg.popup('Chosen infosets:',      \n",
    "                'main, '+ strv )\n",
    "    return v['info']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8de97449",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_database( ):\n",
    "    # Very basic window.  Return values using auto numbered keys\n",
    "\n",
    "    layout = [\n",
    "        [sg.Text('Please enter Database and Collection names')],\n",
    "        [sg.Text('Connection string', size=(15, 1)), sg.InputText()],\n",
    "        [sg.Text('Database', size=(15, 1)), sg.InputText()],\n",
    "        [sg.Text('collection', size=(15, 1)), sg.InputText()],\n",
    "        [sg.Submit(), sg.Cancel()]\n",
    "    ]\n",
    "\n",
    "    window = sg.Window('Database entry window', layout)\n",
    "    event, values = window.read()\n",
    "    window.close()\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f093b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reads the list of titles from the file and deletes the first two letters of the code\n",
    "def get_data(filename):\n",
    "    try:\n",
    "        titles = pd.read_csv(filename, usecols=[0], names=['_id'])\n",
    "        titles['_id'] = titles['_id'].str.slice_replace(start=0, stop=2, repl='')\n",
    "    except:\n",
    "        sg.popup(\"The dataset is incorrect, exit\")\n",
    "        raise SystemExit(\"Cancelling: The dataset is incorrect\")\n",
    "    return titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5daf093e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify(DataObj):\n",
    "    idoc = {}\n",
    "    tag=''\n",
    "    if isinstance(DataObj, imdb.Person.Person):\n",
    "        tag = 'nm'\n",
    "    elif isinstance(DataObj, imdb.Movie.Movie):\n",
    "        tag = 'tt'\n",
    "    elif isinstance(DataObj, imdb.Company.Company):\n",
    "        tag = 'co'\n",
    "    else:\n",
    "        # insert here exception-handling\n",
    "        pass    \n",
    "    ID = DataObj.getID()\n",
    "    idoc['_id'] = tag+str(ID)\n",
    "    #idoc['id_'] = ID\n",
    "    return idoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6404f87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(DataObj):\n",
    "    document = {}\n",
    "\n",
    "    classes = (\n",
    "        imdb.Person.Person,\n",
    "        imdb.Movie.Movie,\n",
    "        imdb.Company.Company)\n",
    "\n",
    "    for key in DataObj.keys():\n",
    "\n",
    "        if type(DataObj[key]) is dict:\n",
    "            document[key] = convert(DataObj[key])\n",
    "            \n",
    "\n",
    "        elif type(DataObj[key]) is list:\n",
    "            document.update(identify(DataObj))\n",
    "            values = DataObj[key]\n",
    "\n",
    "            if len(values) == 0:\n",
    "                continue\n",
    "\n",
    "            sample = values[0]\n",
    "\n",
    "            if type(sample) in classes:\n",
    "                val = [x.data for x in values]\n",
    "                for x in val:\n",
    "                    n = val.index(x)\n",
    "                    x.update(identify(values[n]))\n",
    "                    document[key] = val\n",
    "\n",
    "            elif len(values) == 1 and type(values[0]) not in classes:\n",
    "                document[key] = values[0]\n",
    "\n",
    "            elif len(values) == 1 and type(values[0]) in classes:\n",
    "                data = values[0].data\n",
    "                data.update(identify(values[0]))\n",
    "                document[key] = [data]\n",
    "\n",
    "            elif type(sample) in (str, bytes):\n",
    "                document[key] = DataObj[key]\n",
    "\n",
    "        elif type(DataObj[key]) in classes:\n",
    "            (DataObj[key]).data.update(identify(DataObj[key]))\n",
    "            document[key] = convert((DataObj[key]).data)\n",
    "\n",
    "        else:\n",
    "            document[key] = DataObj[key]\n",
    "\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4778468e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_error_message(error_message):\n",
    "    \"\"\"Append error message as a new line at the end of file\"\"\"\n",
    "    # Open the file in append & read mode ('a+')\n",
    "    with open('errors.txt', \"a+\") as file_object:\n",
    "        # Move read cursor to the start of file.\n",
    "        file_object.seek(0)\n",
    "        # If file is not empty then append '\\n'\n",
    "        data = file_object.read(100)\n",
    "        if len(data) > 0:\n",
    "            file_object.write(\"\\n\")\n",
    "        # Append text at the end of file\n",
    "        file_object.write(error_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c4ec8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the filmography file starting from the title identifier (title) \n",
    "#and attributes it (in json format) to the variable movie\n",
    "def get_main(title, infoset):\n",
    "    \n",
    "    try:\n",
    "        mv = ia.get_movie(title, info = infoset)\n",
    "#     except (KeyError):\n",
    "#         new_infoset = copy.copy(infoset)\n",
    "#         new_infoset.remove('episodes')\n",
    "#         mv = ia.get_movie(title, info = new_infoset)\n",
    "    except IMDbError as e:\n",
    "        append_error_message(str(e))\n",
    "        movie = None\n",
    "        return movie\n",
    "    movie = json.dumps(convert(mv))\n",
    "    return movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e721fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applies the previous one for each title identifier contained in the dataframe. \n",
    "#It works in parallel by taking advantage of the available cores.\n",
    "def dask_impl(df, infoset):\n",
    "    # from dask.diagnostics import ProgressBar\n",
    "    # pbar = ProgressBar()\n",
    "    # pbar.register()\n",
    "    return dd.from_pandas(df, npartitions=CPU_COUNT).apply(\n",
    "    lambda row: get_main(\n",
    "        row._id, infoset),\n",
    "    axis=1, \n",
    "    meta=(int)\n",
    "  ).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acbfacb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Non-parallel function alternative to the previous one\n",
    "def apply_impl(df, infoset):\n",
    "    return df.apply(\n",
    "        lambda row: get_main(\n",
    "        row._id, infoset), axis = 1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9eadaf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect(values, coll=None):\n",
    "\n",
    "    client = pymongo.MongoClient(values[0])\n",
    "    db = client[str(values[1])]\n",
    "    collection = db[str(values[2])]\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2eb3fd2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def to_mongo(mov, values):\n",
    "    collection = connect(values)\n",
    "    pyresponse = json.loads(mov)\n",
    "    collection.insert_one(pyresponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a84ea71",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def app(df, values):\n",
    "    try:\n",
    "        dd.from_pandas(df, npartitions=CPU_COUNT).apply(to_mongo, args=(values,), meta=(int)).compute()\n",
    "    except:\n",
    "        sg.popup(\"Something wrong with the connection, exit\")\n",
    "        raise SystemExit(\"Cancelling: Something wrong with the connection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3980b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    sg.theme('Material1')      # Add some color to the window\n",
    "\n",
    "    filename = read_table()\n",
    "    titles = get_data(filename)\n",
    "    values = get_database()\n",
    "    infoset = get_infoset()\n",
    "    infoset.insert(0, 'main')\n",
    "    \n",
    "    layout = [  [sg.Text('Below you can see the download progress:')],\n",
    "    [sg.Output(size=(60,3), key='-OUTPUT-')]    ]\n",
    "    window = sg.Window('Window Title', layout, finalize = True)\n",
    "    \n",
    "    df = dask_impl(titles, infoset)\n",
    "    window.close()\n",
    "    df.dropna(inplace=True)\n",
    "    app(df, values)\n",
    "    sg.popup(\"Operation completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "568649ec",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c63f844",
   "metadata": {},
   "source": [
    "### Data visualization (30 minutes)\n",
    "- Tools and techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8fa118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregation(field):\n",
    "    #Inserire qui la stringa di connessione a MongoDB con il proprio nome utente e password\n",
    "    client = pymongo.MongoClient('<Atlas connection string>')\n",
    "    #Indicare qui il nome del database e della collection generati da IMDb2Mongo\n",
    "    result = client['<Database>']['<Collection>'].aggregate([\n",
    "    {\n",
    "        '$match': {\n",
    "            f'{field}': {\n",
    "                '$exists': True, \n",
    "                '$ne': []\n",
    "            }\n",
    "        }\n",
    "    }, {\n",
    "        '$project': {\n",
    "            '_id': 0, \n",
    "            f'{field}': 1, \n",
    "            'year': 1\n",
    "        }\n",
    "    }, {\n",
    "        '$unwind': {\n",
    "            'path': f'${field}'\n",
    "        }\n",
    "    }, {\n",
    "        '$addFields':{\n",
    "        f\"{field}.code\": f'${field}._id',\n",
    "        f\"{field}.role\": f'{field}',\n",
    "        f\"{field}.year\": '$year'\n",
    "    }\n",
    "    }, {\n",
    "        '$replaceRoot': {\n",
    "            'newRoot': f'${field}'\n",
    "        }\n",
    "    },{\n",
    "        '$project': {\n",
    "            '_id': 0\n",
    "        }\n",
    "    },{\n",
    "        '$merge': {\n",
    "# Inserire qui il nome della Crew Collection che verrà salvata su MongoDB\n",
    "            'into':'<Crew Collection Name>'\n",
    "        }\n",
    "    }\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87341de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # I fields vanno controllati e eventualmente aggiunti i nuovi\n",
    "    fields = ['art department', 'art direction', 'assistant director', 'camera and electrical department', 'cast', 'casting department', 'casting director', 'cinematographer', 'composer', 'costume department', 'costume designer', 'creator', 'director', 'editor', 'editorial department', 'location management', 'make up', 'miscellaneous crew', 'music department', 'producer', 'production design', 'production manager', 'script department', 'set decoration', 'sound crew', 'special effects', 'stunt performer', 'visual effects', 'writer']\n",
    "    for field in fields:\n",
    "        aggregation(field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcf93e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d519671c",
   "metadata": {},
   "source": [
    "## Genderize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba06d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "crew = pd.read_csv(\"Crew_significant03112022.csv\", sep=',')\n",
    "gender = pd.read_csv(\"names_gendered_rev_ita.14.09.2022.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a419fbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "crew.rename(columns = {'code':'nconst'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ad9ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "crew_gender = crew.merge(gender, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee4efeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "crew_gender.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51772b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "crew_gender.drop('name', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa07907a",
   "metadata": {},
   "outputs": [],
   "source": [
    "crew_gender.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43805027",
   "metadata": {},
   "outputs": [],
   "source": [
    "crew_gender.to_csv(r\"crew_significant_gender.03.11.2022.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0c1d9d",
   "metadata": {},
   "source": [
    "### Q&A and discussion (30 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5262eccf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
